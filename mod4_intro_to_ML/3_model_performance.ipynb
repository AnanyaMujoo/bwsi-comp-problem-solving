{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7e2a42",
   "metadata": {},
   "source": [
    "# Intro to ML: Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f9e78",
   "metadata": {},
   "source": [
    "## Motivation \n",
    "\n",
    "How do we know we can trust our models?\n",
    "\n",
    "Say we're working with a model that predicts weather a patient has cancer. If a model predicts that a pateint has cancer, we would want to know how sure the model is of this prediction. \n",
    "\n",
    "Models make predictions but our confidence/trust in a models ability to make **accurate** predictions comes from evaluating how well they perform on **practice (training) cases** and **new (test) cases**. \n",
    "\n",
    "This homework explores methods to assess the accuracy of model predictions. \n",
    "\n",
    "Key idea:\n",
    "“A model is only as good as the data it’s trained on — and as the metrics we use to judge it.”\n",
    "\n",
    "Consider the following analogy: \n",
    "\n",
    "A students performance on a calculus test is directly related to the amount of time and the types of problems they practiced. It's best to evaluate a students calculus abilities using calculus problems! Don't use a geometry test to test abilities in calculus!\n",
    "\n",
    "In this analogy: \n",
    "\n",
    "1. model = student\n",
    "2. training data = the caluclus problems student practiced\n",
    "3. metrics to test model performance = calculus test \n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://dataedo-website.s3.amazonaws.com/cartoon/machine_learning.png?1654170935\" width = \"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07217c13",
   "metadata": {},
   "source": [
    "# Question 1: In your own words, given the analogy above, define Machine Learning.\n",
    "\n",
    "Hopefully this reinforces the idea of **Machine Learning**: an algorithm learning a relationship in the data so that it can make predictions when handed new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27148a53",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is defined as: \n",
    "\n",
    "$Accuracy=\\frac{Correct Predictions}{Total Predictions}$ \n",
    "\n",
    "Though it's an intuitive first choice for a model performance metric it can be misleading when working with multi-class datasets. For example, predicting “does not have cancer\" for everyone when 99% do not have cancer is not very helpful. We don't really know how well the model does at predicting in situations when someone does have cancer!\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.licdn.com/dms/image/v2/C5612AQGpdX8HSIAEeA/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1610873977828?e=2147483647&v=beta&t=gnytCodqB3H_WcT0Zz3NpDqHfbNNIL_OTctWWcB1iKE\" width = \"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f36b4a5",
   "metadata": {},
   "source": [
    "## The Data Life Cycle in ML\n",
    "\n",
    "A schematic of the training (learning) and testing (evaluation) procedures for a ML model. \n",
    "<p align=\"left\">\n",
    "    <img src = \"https://www.machinelearningplus.com/wp-content/uploads/2022/12/train_test_split-procedure.jpg\" width = \"800\">\n",
    "</p>\n",
    "\n",
    "Imagine you’re studying for a test. If you only practice using the exact same questions that will be on the test, will you really know the material? Probably not — you’d just memorize the answers.\n",
    "\n",
    "Key idea: We want our models to learn patterns in data, not memorize the training examples. That’s why we divide our data into parts — to test if the model can handle new, unseen examples.\n",
    "\n",
    "## Training and Testing\n",
    "When we train a machine learning model:\n",
    "1. Training set: Used to teach the model patterns in the data.\n",
    "2. Testing set: Used to evaluate how well the model performs on new data.\n",
    "\n",
    "Typical we allocate 70–80% of the data for training and 20–30% for testing. \n",
    "\n",
    "### Generalization\n",
    "\n",
    "Generalization is a model’s ability to make accurate predictions on data it has never seen before. Models that don't generalize well fall into one of two categories, over or underfitting: \n",
    "\n",
    "| Type                       | Description                                                      | Example Behavior                            | \n",
    "| -------------------------- | ---------------------------------------------------------------- |  ------------------------------------------ |\n",
    "| Overfitting                | Overfitting\tModel is too complex, memorizes noise          | Perfect on training data, fails on new data        | \n",
    "| Underfitting               |  Model is too simple, doesn’t capture underlying pattern               | Straight line through nonlinear data | \t\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:1400/1*pXJJTOS0f0dqgnlleP10aA.jpeg\" width = \"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14202e0",
   "metadata": {},
   "source": [
    "# Question 2: Why do you think it's common practice to split the data 80% train and 20% test instead of 50% train and 50% test?\n",
    "\n",
    "Intuitively, you're limiting the amount of learning the algorithm is doing! This is bad, we want the algorithm to learn as much as it can before we use it for real-world applications. \n",
    "\n",
    "Interestingly, this is just a convention that was adopted in the ML environements -- it's not the result of any analytical proof or optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36dff0",
   "metadata": {},
   "source": [
    "Below we train a classification algorithm to label patients as 'has cancer' or 'does not have cancer'. The train and testing accuracy is printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9626373626373627\n",
      "Testing Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# split data into train (80%) and test (20%); random state for reproducibility \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# train a logistic regression model -- it's typically used to answer binary classification problems\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on both train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31dd32",
   "metadata": {},
   "source": [
    "# Question 3: So we're getting 95%+ accuracy in our testing. Does this alone tell us our model is great?\n",
    "\n",
    "No! We want to know how accurate the model does predicting **by category**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c78a26",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "If a model predicts whether a patient has cancer or not, what does it mean if it’s 95%+ accurate? Is that good? We can't really tell! What we really want to know is: \n",
    "1. How accurate is the model at correctly predicting the pateint has cancer and \n",
    "2. How accurate is the model at correctly predicting the pateint does not have cancer\n",
    "\n",
    "That’s where the classification report comes in — it gives us a deeper look at model performance **by class**. The classification report summarizes several important metrics for each class in a classification problem. It shows how well the model identifies each category, it includes:\n",
    "\n",
    "| Metric    | Meaning                                                   | Formula                               |\n",
    "| --------- | --------------------------------------------------------- | ------------------------------------- |\n",
    "| Precision | Of all items predicted as class X, how many were correct? | $\\frac{TP}{TP + FP}$                |\n",
    "| Recall    | Of all true class X items, how many did we find?          | $\\frac{TP}{TP + FN}$                |\n",
    "| F1-Score  | Balance between precision and recall.                     | $2 \\times \\frac{P \\times R}{P + R}$ |\n",
    "| Support   | Number of true instances of each class.                   | Count                                 |\n",
    "\n",
    "Let Positive = 'to not have cancer' and Negative = 'to have cancer'\n",
    "- FP: False Positive\n",
    "    - ex: someone with cancer is told they do not have cancer\n",
    "- TP: True Positive \n",
    "    - ex: someone that does not have cancer is told they do not have cancer\n",
    "- FN: False Negative\n",
    "    - ex: someone without cancer is told they have cancer\n",
    "- TN: True Negative\n",
    "    - ex: someone with cancer is told they have cancer\n",
    "\n",
    "These Positive and Negative labels are summarized in the form of a confusion matrix, shown below: \n",
    "<p align=\"left\">\n",
    "    <img src = \"https://www.blog.trainindata.com/wp-content/uploads/2024/09/confusion-matrix-1.png\" width = \"800\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf0070",
   "metadata": {},
   "source": [
    "The confusion matrix for the algorithm we trained above is shown below. The matrix depicts the results of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d0f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[39  4]\n",
      " [ 1 70]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.91      0.94        43\n",
      "      benign       0.95      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred), \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=load_breast_cancer().target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e0562",
   "metadata": {},
   "source": [
    "# Question 4: What could you say about the model we trained above if the precision and recall were both 1? Make quantitative arguments about the values of FN and FP to support your answer.\n",
    "\n",
    "It's a perfect model! It would mean we have an FP=0=FN. This means we don't ever falsely label people with cancer or falsely label people without cancer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95373e25",
   "metadata": {},
   "source": [
    "# Question 5: Sometimes it's not possible to increase precision without decreasing recall (and vice-versa)? Provide a conceptual argument for why this might be the case. Assume there's a way to force the model to maximize precision (or recall) upon the users request (there is). \n",
    "\n",
    "If we want to maximize precision then we want to limit the number of FP, the number of predictions that do not label patients with cancer. Doing so will naturally push models to label more patients as 'has cancer'. But labeling more patients as 'has cancer' will increase the chances of a the model incorrectly labeling a patient with cancer, therefore increasing the number of FN. \n",
    "\n",
    "You can make a similar argument if you choose to maximize recall instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f845524",
   "metadata": {},
   "source": [
    "## More Testing, Cross Validation  \n",
    "\n",
    "Sometimes, a single train/test split isn’t enough. Why? Because your model’s performance might depend on how the data happened to be split.\n",
    "\n",
    "Consider the following example: \n",
    "\n",
    "If you only take one practice test, it might not reflect your true skill. But if you take five different versions and average your scores, you’ll get a better estimate of how you’ll perform on the real exam.\n",
    "\n",
    "This is what cross-validation does!\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://miro.medium.com/1*GhKMAUmi4bfFiEwZCPlDsA.png\" width = \"800\">\n",
    "</p>\n",
    "\n",
    "K-Fold Cross-Validation Psuedo Code: \n",
    "\n",
    "1. Split the data into K parts (folds).\n",
    "2. Train on K–1 folds, test on the remaining one.\n",
    "3. Repeat K times, each time using a different fold as the test set.\n",
    "4. Average the performance scores.\n",
    "\n",
    "Notice that cross validation uses all data for both training and testing (just not at the same time). This allows us to produce a more reliable estimate of model performance.\n",
    "\n",
    "Types of Cross-Validation (CV)\n",
    "1. K-Fold CV: Standard method as described above.\n",
    "2. Stratified K-Fold: Keeps class proportions balanced.\n",
    "    - example: Given an image of a solid color, a machine learning model predicts the color of an image as \"orange\", \"purple\", or \"yellow\". Stratified K-fold splits up the data so that in each fold there is an equal amount of \"orange\", \"purple\", or \"yellow\" images for **both* training and testing. \n",
    "3. Leave-One-Out CV: Each sample is its own test set (expensive but precise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d460821",
   "metadata": {},
   "source": [
    "# Question 6: (Bonus for Cool Nerds Only) If you're working with a classification model, why is it bad if your data is imbalanced? What would you need to do if your data is imbalanced but you still want to perform CV?\n",
    "If you're data is imbalanced then it's difficult for the model to learn when you're data maps to a particular category/class. You'll need to employ some oversampling methods. SMOTE is a go-to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: [1.         1.         0.94444444 0.97142857 1.        ]\n",
      "Mean accuracy: 0.9831746031746033\n",
      "Standard deviation: 0.02230370548603213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# another type of algorithm commonly used for classification problems \n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"Cross-validation accuracies:\", scores)\n",
    "print(\"Mean accuracy:\", np.mean(scores))\n",
    "print(\"Standard deviation:\", np.std(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97883052",
   "metadata": {},
   "source": [
    "# Question 7: I committed a carnal sin when evaluating the performance of this algorithm. What was it?\n",
    "In the example above I used accuracy as a metric for model performance but remember in practice you should not use this. Instead you should assess the model's performance by cateogry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2dbeb",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:orange;\">Happy Halloween!</span>\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://static.wikia.nocookie.net/p__/images/1/1e/SpookleyTheSquarePumpkin.webp/revision/latest/thumbnail/width/360/height/360?cb=20250921145457&path-prefix=protagonist\" width = \"800\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dash_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
