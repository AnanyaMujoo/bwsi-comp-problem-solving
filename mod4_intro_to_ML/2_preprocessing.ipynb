{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-0001",
   "metadata": {},
   "source": [
    "# Intro to ML: Preprocessing Data (High School)\n",
    "**Time:** ~60 minutes | **Level:** Beginner | **Tools:** Python, pandas, scikit-learn, matplotlib\n",
    "\n",
    "> Goal: Understand why and how we clean, scale, encode, and reduce data **before** training ML models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives-0002",
   "metadata": {},
   "source": [
    "## Learning Objectives (What you'll be able to do)\n",
    "- Explain why preprocessing matters for ML.\n",
    "- Clean a small dataset (fix missing values/outliers).\n",
    "- **Scale** numeric features (MinMax, Standard, Robust).\n",
    "- **Encode** categorical features (One-Hot vs. Ordinal).\n",
    "- Avoid **data leakage** with train/test split and Pipelines.\n",
    "- Use **PCA** to reduce dimensions and visualize data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agenda-0003",
   "metadata": {},
   "source": [
    "## Agenda & Timing\n",
    "- (05m) Warm-up: Where data misleads us\n",
    "- (06m) When & Why Preprocess?\n",
    "- (08m) Cleaning & Missing Values\n",
    "- (10m) Scaling (Standard vs MinMax vs Robust) + demo\n",
    "- (10m) Encoding (One-Hot vs Ordinal) + demo\n",
    "- (05m) Train/Test Split & Data Leakage (Pipelines)\n",
    "- (08m) PCA (dimension reduction) + demo\n",
    "- (03m) Quick Quiz + Wrap-up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warmup-0004",
   "metadata": {},
   "source": [
    "## Warm-up (5m): Discuss with a partner\n",
    "1. If a model predicts height from **shoe size** and **favorite color**, which feature is useless? Why?\n",
    "2. If one feature is in **dollars** (0–100,000) and another is **ages** (0–18), which might dominate a distance-based model like *k-NN*?\n",
    "3. Should we give numbers to categories (e.g., Red=1, Blue=2, Green=3)? When is that okay vs. misleading?\n",
    "\n",
    "> **Takeaway:** Bad preprocessing can make a good model look bad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-0005",
   "metadata": {},
   "source": [
    "## When and Why Preprocess? (6m)\n",
    "- Real data is messy: missing values, typos, outliers.\n",
    "- Many ML algorithms expect numbers on similar scales.\n",
    "- Categorical data (city, color) must be turned into numbers **carefully**.\n",
    "- Preprocessing can make training **faster** and **more accurate**.\n",
    "\n",
    "**Analogy:** Running a race in muddy shoes vs. clean shoes — same runner, different performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-0006",
   "metadata": {},
   "source": [
    "## Cleaning & Missing Values (8m)\n",
    "Common steps:\n",
    "- **Detect missing values** and impute (mean/median/mode) or drop.\n",
    "- **Fix outliers** (clip or use RobustScaler).\n",
    "- **Consistency**: units (cm vs. m), text cases (NY vs. New York).\n",
    "\n",
    "**Engaging Q:** If 'height' is missing for 10 students, is it better to drop those rows or fill with the class median? Why?\n",
    "\n",
    "<details><summary>Answer idea</summary>\n",
    "Dropping loses data (bad if many). Median keeps sample size and is robust to outliers.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-clean-0007",
   "metadata": {},
   "source": [
    "# Demo: Create a tiny messy dataset and clean it\n",
    "import numpy as np, pandas as pd\n",
    "np.random.seed(0)\n",
    "df = pd.DataFrame({\n",
    "    'age': [15, 16, 16, np.nan, 17, 15, 16, 100],   # 100 is an outlier\n",
    "    'height_cm': [160, 165, np.nan, 170, 168, 159, 172, 300], # 300 is an outlier\n",
    "    'favorite_color': ['red','blue','green','blue','red', None, 'green','red'],\n",
    "    'passed_exam': [1,0,1,1,0,1,0,1]\n",
    "})\n",
    "print('Raw data:')\n",
    "display(df)\n",
    "\n",
    "# Simple cleaning: impute numeric with median, categorical with mode\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['height_cm'] = df['height_cm'].fillna(df['height_cm'].median())\n",
    "df['favorite_color'] = df['favorite_color'].fillna(df['favorite_color'].mode()[0])\n",
    "\n",
    "# Clip outliers to percentile bounds\n",
    "for col in ['age','height_cm']:\n",
    "    low, high = df[col].quantile([0.01, 0.99])\n",
    "    df[col] = df[col].clip(lower=low, upper=high)\n",
    "\n",
    "print('Cleaned data:')\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-0008",
   "metadata": {},
   "source": [
    "## Scaling (10m)\n",
    "Why scale? Distance-based models (k-NN, k-means) and gradient-based models (logistic regression, neural nets) behave better when features are on similar ranges.\n",
    "\n",
    "**Common scalers:**\n",
    "- **StandardScaler**: z-score (mean 0, std 1).\n",
    "- **MinMaxScaler**: rescales to [0, 1].\n",
    "- **RobustScaler**: uses median/IQR (good for outliers).\n",
    "\n",
    "**Engaging Q:** If we change units from cm to m (divide by 100), do we need scaling? What if one feature is in dollars (0–100,000)?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-scale-0009",
   "metadata": {},
   "source": [
    "# Demo: k-NN accuracy before/after scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = df[['age','height_cm']].values\n",
    "y = df['passed_exam'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# Unscaled pipeline\n",
    "knn_unscaled = Pipeline([('knn', KNeighborsClassifier(n_neighbors=3))])\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "acc_unscaled = knn_unscaled.score(X_test, y_test)\n",
    "\n",
    "# Scaled pipeline (StandardScaler)\n",
    "knn_scaled = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=3))])\n",
    "knn_scaled.fit(X_train, y_train)\n",
    "acc_scaled = knn_scaled.score(X_test, y_test)\n",
    "\n",
    "print(f'k-NN accuracy (unscaled): {acc_unscaled:.2f}')\n",
    "print(f'k-NN accuracy (scaled):   {acc_scaled:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding-0010",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables (10m)\n",
    "We must turn text categories into numbers **without creating fake order**.\n",
    "\n",
    "**Two strategies:**\n",
    "- **One-Hot Encoding** (unordered categories): red → [1,0,0], blue → [0,1,0], green → [0,0,1]\.\n",
    "- **Ordinal Encoding** (true order): size small=0 < medium=1 < large=2.\n",
    "\n",
    "**Engaging Q:** Why is assigning red=1, blue=2, green=3 a problem for models?\n",
    "\n",
    "<details><summary>Answer idea</summary>\n",
    "It implies green > blue > red numerically, creating a fake order and distances.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-encode-0011",
   "metadata": {},
   "source": [
    "# Demo: One-Hot vs Ordinal\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Add a (fake) ordered feature\n",
    "df2 = df.copy()\n",
    "df2['shirt_size'] = ['S','M','L','M','S','L','M','S']\n",
    "\n",
    "numeric = ['age','height_cm']\n",
    "categorical_unordered = ['favorite_color']\n",
    "categorical_ordered = ['shirt_size']\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric),\n",
    "    ('cat_unordered', OneHotEncoder(handle_unknown='ignore'), categorical_unordered),\n",
    "    ('cat_ordered', OrdinalEncoder(categories=[['S','M','L']]), categorical_ordered)\n",
    "])\n",
    "\n",
    "pipe = Pipeline([('prep', preprocess), ('clf', LogisticRegression(max_iter=1000))])\n",
    "X = df2[numeric + categorical_unordered + categorical_ordered]\n",
    "y = df2['passed_exam']\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "pipe.fit(Xtr, ytr)\n",
    "print('Logistic Regression accuracy:', pipe.score(Xte, yte))\n",
    "\n",
    "# Show transformed feature names\n",
    "ohe = pipe.named_steps['prep'].named_transformers_['cat_unordered']\n",
    "print('One-hot categories for favorite_color:', list(ohe.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leakage-0012",
   "metadata": {},
   "source": [
    "## Train/Test Split & Data Leakage (5m)\n",
    "**Data leakage** happens when information from the test set sneaks into training (e.g., scaling on all data before splitting).\n",
    "\n",
    "**Rule:** Always split first, or use a **Pipeline** so that scaling/encoding happen inside cross-validation.\n",
    "\n",
    "**Engaging Q:** Why is it cheating to compute the mean on the whole dataset before splitting and then use it to scale the train/test sets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca-0013",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) (8m)\n",
    "PCA finds new axes that capture the most variance (information) and lets us **compress** data.\n",
    "\n",
    "- Good for visualization (2D plot of high-D data).\n",
    "- Can speed up training and reduce noise.\n",
    "\n",
    "**Analogy:** Taking a selfie from the most flattering angle — same face, better view.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-pca-0014",
   "metadata": {},
   "source": [
    "# Demo: PCA on a synthetic 3D dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=3, n_features=3, cluster_std=1.8, random_state=42)\n",
    "pca = PCA(n_components=2)\n",
    "X2 = pca.fit_transform(X)\n",
    "print('Explained variance ratio (2 PCs):', pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X2[:,0], X2[:,1], c=y)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA: 3D data projected to 2D')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engage-0015",
   "metadata": {},
   "source": [
    "## Think–Pair–Share (sprinkled throughout)\n",
    "- **Scaling:** Would a decision tree care about scaling? Why or why not?\n",
    "- **Encoding:** When is *ordinal* encoding appropriate? Give 2 examples.\n",
    "- **PCA:** If PCA reduces features from 50 to 2, what do we lose and what do we keep?\n",
    "- **Cleaning:** How would you handle a row with *all* missing values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz-0016",
   "metadata": {},
   "source": [
    "## Quick Quiz (3m)\n",
    "1. Which scaler is most robust to outliers?\n",
    "   - A) MinMax  B) Standard  C) Robust  D) None\n",
    "2. Which encoding is safe for colors?\n",
    "   - A) Ordinal  B) One-Hot  C) Label with numbers  D) None\n",
    "3. True/False: Always scale **before** train/test split.\n",
    "\n",
    "<details><summary>Answers</summary>\n",
    "1: C, 2: B, 3: False (use Pipeline or fit scaler only on train).\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-0017",
   "metadata": {},
   "source": [
    "## Mini-Project (optional, homework)\n",
    "Use any small dataset (or create one) with:\n",
    "- 2+ numeric columns (e.g., age, study_hours)\n",
    "- 1+ categorical column (e.g., favorite_subject)\n",
    "- 1 target (e.g., passed_exam)\n",
    "\n",
    "**Tasks:**\n",
    "1. Clean missing values and outliers.\n",
    "2. Build a Pipeline with scaling (for numeric) and one-hot (for categorical).\n",
    "3. Train logistic regression; report accuracy on a test set.\n",
    "4. Try PCA (2 components) and re-train; compare accuracy.\n",
    "5. Write 3 sentences on what preprocessing changed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {"name": "python"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
