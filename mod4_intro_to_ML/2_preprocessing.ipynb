{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-0001",
   "metadata": {},
   "source": [
    "# Intro to ML: Preprocessing Data (High School)
",
    "**Time:** ~60 minutes | **Level:** Beginner | **Tools:** Python, pandas, scikit-learn, matplotlib

",
    "> Goal: Understand why and how we clean, scale, encode, and reduce data **before** training ML models.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objectives-0002",
   "metadata": {},
   "source": [
    "## Learning Objectives (What you'll be able to do)
",
    "- Explain why preprocessing matters for ML.
",
    "- Clean a small dataset (fix missing values/outliers).
",
    "- **Scale** numeric features (MinMax, Standard, Robust).
",
    "- **Encode** categorical features (One-Hot vs. Ordinal).
",
    "- Avoid **data leakage** with train/test split and Pipelines.
",
    "- Use **PCA** to reduce dimensions and visualize data.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agenda-0003",
   "metadata": {},
   "source": [
    "## Agenda & Timing
",
    "- (05m) Warm-up: Where data misleads us
",
    "- (06m) When & Why Preprocess?
",
    "- (08m) Cleaning & Missing Values
",
    "- (10m) Scaling (Standard vs MinMax vs Robust) + demo
",
    "- (10m) Encoding (One-Hot vs Ordinal) + demo
",
    "- (05m) Train/Test Split & Data Leakage (Pipelines)
",
    "- (08m) PCA (dimension reduction) + demo
",
    "- (03m) Quick Quiz + Wrap-up
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warmup-0004",
   "metadata": {},
   "source": [
    "## Warm-up (5m): Discuss with a partner
",
    "1. If a model predicts height from **shoe size** and **favorite color**, which feature is useless? Why?
",
    "2. If one feature is in **dollars** (0–100,000) and another is **ages** (0–18), which might dominate a distance-based model like *k-NN*?
",
    "3. Should we give numbers to categories (e.g., Red=1, Blue=2, Green=3)? When is that okay vs. misleading?

",
    "> **Takeaway:** Bad preprocessing can make a good model look bad.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-0005",
   "metadata": {},
   "source": [
    "## When and Why Preprocess? (6m)
",
    "- Real data is messy: missing values, typos, outliers.
",
    "- Many ML algorithms expect numbers on similar scales.
",
    "- Categorical data (city, color) must be turned into numbers **carefully**.
",
    "- Preprocessing can make training **faster** and **more accurate**.

",
    "**Analogy:** Running a race in muddy shoes vs. clean shoes — same runner, different performance.
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-0006",
   "metadata": {},
   "source": [
    "## Cleaning & Missing Values (8m)
",
    "Common steps:
",
    "- **Detect missing values** and impute (mean/median/mode) or drop.
",
    "- **Fix outliers** (clip or use RobustScaler).
",
    "- **Consistency**: units (cm vs. m), text cases (NY vs. New York).

",
    "**Engaging Q:** If 'height' is missing for 10 students, is it better to drop those rows or fill with the class median? Why?

",
    "<details><summary>Answer idea</summary>
",
    "Dropping loses data (bad if many). Median keeps sample size and is robust to outliers.
",
    "</details>
"
   ]}
 ,
  {
   "cell_type": "code",
   "id": "code-clean-0007",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demo: Create a tiny messy dataset and clean it
",
    "import numpy as np, pandas as pd
",
    "from IPython.display import display
",
    "np.random.seed(0)
",
    "df = pd.DataFrame({
",
    "    'age': [15, 16, 16, np.nan, 17, 15, 16, 100],   # 100 is an outlier
",
    "    'height_cm': [160, 165, np.nan, 170, 168, 159, 172, 300], # 300 is an outlier
",
    "    'favorite_color': ['red','blue','green','blue','red', None, 'green','red'],
",
    "    'passed_exam': [1,0,1,1,0,1,0,1]
",
    "})
",
    "print('Raw data:')
",
    "display(df)
",
    "
",
    "# Simple cleaning: impute numeric with median, categorical with mode
",
    "df['age'] = df['age'].fillna(df['age'].median())
",
    "df['height_cm'] = df['height_cm'].fillna(df['height_cm'].median())
",
    "df['favorite_color'] = df['favorite_color'].fillna(df['favorite_color'].mode()[0])
",
    "
",
    "# Clip outliers to percentile bounds
",
    "for col in ['age','height_cm']:
",
    "    low, high = df[col].quantile([0.01, 0.99])
",
    "    df[col] = df[col].clip(lower=low, upper=high)
",
    "
",
    "print('Cleaned data:')
",
    "display(df)
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-0008",
   "metadata": {},
   "source": [
    "## Scaling (10m)
",
    "Why scale? Distance-based models (k-NN, k-means) and gradient-based models (logistic regression, neural nets) behave better when features are on similar ranges.

",
    "**Common scalers:**
",
    "- **StandardScaler**: z-score (mean 0, std 1).
",
    "- **MinMaxScaler**: rescales to [0, 1].
",
    "- **RobustScaler**: uses median/IQR (good for outliers).

",
    "**Engaging Q:** If we change units from cm to m (divide by 100), do we need scaling? What if one feature is in dollars (0–100,000)?
"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-scale-0009",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demo: k-NN accuracy before/after scaling
",
    "from sklearn.model_selection import train_test_split
",
    "from sklearn.neighbors import KNeighborsClassifier
",
    "from sklearn.preprocessing import StandardScaler
",
    "from sklearn.pipeline import Pipeline
",
    "
",
    "X = df[['age','height_cm']].values
",
    "y = df['passed_exam'].values
",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)
",
    "
",
    "# Unscaled pipeline
",
    "knn_unscaled = Pipeline([('knn', KNeighborsClassifier(n_neighbors=3))])
",
    "knn_unscaled.fit(X_train, y_train)
",
    "acc_unscaled = knn_unscaled.score(X_test, y_test)
",
    "
",
    "# Scaled pipeline (StandardScaler)
",
    "knn_scaled = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=3))])
",
    "knn_scaled.fit(X_train, y_train)
",
    "acc_scaled = knn_scaled.score(X_test, y_test)
",
    "
",
    "print(f'k-NN accuracy (unscaled): {acc_unscaled:.2f}')
",
    "print(f'k-NN accuracy (scaled):   {acc_scaled:.2f}')
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding-0010",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables (10m)
",
    "We must turn text categories into numbers **without creating fake order**.

",
    "**Two strategies:**
",
    "- **One-Hot Encoding** (unordered categories): red → [1,0,0], blue → [0,1,0], green → [0,0,1].
",
    "- **Ordinal Encoding** (true order): size small=0 < medium=1 < large=2.

",
    "**Engaging Q:** Why is assigning red=1, blue=2, green=3 a problem for models?

",
    "<details><summary>Answer idea</summary>
",
    "It implies green > blue > red numerically, creating a fake order and distances.
",
    "</details>
"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-encode-0011",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demo: One-Hot vs Ordinal
",
    "from sklearn.compose import ColumnTransformer
",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
",
    "from sklearn.linear_model import LogisticRegression
",
    "from sklearn.model_selection import train_test_split
",
    "from sklearn.pipeline import Pipeline
",
    "from sklearn.preprocessing import StandardScaler
",
    "
",
    "# Add a (fake) ordered feature
",
    "df2 = df.copy()
",
    "df2['shirt_size'] = ['S','M','L','M','S','L','M','S']
",
    "
",
    "numeric = ['age','height_cm']
",
    "categorical_unordered = ['favorite_color']
",
    "categorical_ordered = ['shirt_size']
",
    "
",
    "preprocess = ColumnTransformer([
",
    "    ('num', StandardScaler(), numeric),
",
    "    ('cat_unordered', OneHotEncoder(handle_unknown='ignore'), categorical_unordered),
",
    "    ('cat_ordered', OrdinalEncoder(categories=[['S','M','L']]), categorical_ordered)
",
    "])
",
    "
",
    "pipe = Pipeline([('prep', preprocess), ('clf', LogisticRegression(max_iter=1000))])
",
    "X = df2[numeric + categorical_unordered + categorical_ordered]
",
    "y = df2['passed_exam']
",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.4, random_state=1)
",
    "pipe.fit(Xtr, ytr)
",
    "print('Logistic Regression accuracy:', pipe.score(Xte, yte))
",
    "
",
    "# Show transformed feature names for the one-hot part
",
    "ohe = pipe.named_steps['prep'].named_transformers_['cat_unordered']
",
    "print('One-hot categories for favorite_color:', list(ohe.get_feature_names_out(categorical_unordered)))
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leakage-0012",
   "metadata": {},
   "source": [
    "## Train/Test Split & Data Leakage (5m)
",
    "**Data leakage** happens when information from the test set sneaks into training (e.g., scaling on all data before splitting).

",
    "**Rule:** Always split first, or use a **Pipeline** so that scaling/encoding happen inside cross-validation.

",
    "**Engaging Q:** Why is it cheating to compute the mean on the whole dataset before splitting and then use it to scale the train/test sets?
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca-0013",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) (8m)
",
    "PCA finds new axes that capture the most variance (information) and lets us **compress** data.

",
    "- Good for visualization (2D plot of high-D data).
",
    "- Can speed up training and reduce noise.

",
    "**Analogy:** Taking a selfie from the most flattering angle — same face, better view.
"
   ]
  },
  {
   "cell_type": "code",
   "id": "code-pca-0014",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demo: PCA on a synthetic 3D dataset
",
    "from sklearn.datasets import make_blobs
",
    "from sklearn.decomposition import PCA
",
    "import matplotlib.pyplot as plt
",
    "
",
    "X, y = make_blobs(n_samples=300, centers=3, n_features=3, cluster_std=1.8, random_state=42)
",
    "pca = PCA(n_components=2)
",
    "X2 = pca.fit_transform(X)
",
    "print('Explained variance ratio (2 PCs):', pca.explained_variance_ratio_)
",
    "
",
    "plt.figure(figsize=(6,4))
",
    "plt.scatter(X2[:,0], X2[:,1], c=y)
",
    "plt.xlabel('PC1')
",
    "plt.ylabel('PC2')
",
    "plt.title('PCA: 3D data projected to 2D')
",
    "plt.show()
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engage-0015",
   "metadata": {},
   "source": [
    "## Think–Pair–Share (sprinkled throughout)
",
    "- **Scaling:** Would a decision tree care about scaling? Why or why not?
",
    "- **Encoding:** When is *ordinal* encoding appropriate? Give 2 examples.
",
    "- **PCA:** If PCA reduces features from 50 to 2, what do we lose and what do we keep?
",
    "- **Cleaning:** How would you handle a row with *all* missing values?
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz-0016",
   "metadata": {},
   "source": [
    "## Quick Quiz (3m)
",
    "1. Which scaler is most robust to outliers?
",
    "   - A) MinMax  B) Standard  C) Robust  D) None
",
    "2. Which encoding is safe for colors?
",
    "   - A) Ordinal  B) One-Hot  C) Label with numbers  D) None
",
    "3. True/False: Always scale **before** train/test split.

",
    "<details><summary>Answers</summary>
",
    "1: C, 2: B, 3: False (use Pipeline or fit scaler only on train).
",
    "</details>
"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-0017",
   "metadata": {},
   "source": [
    "## Mini-Project (optional, homework)
",
    "Use any small dataset (or create one) with:
",
    "- 2+ numeric columns (e.g., age, study_hours)
",
    "- 1+ categorical column (e.g., favorite_subject)
",
    "- 1 target (e.g., passed_exam)

",
    "**Tasks:**
",
    "1. Clean missing values and outliers.
",
    "2. Build a Pipeline with scaling (for numeric) and one-hot (for categorical).
",
    "3. Train logistic regression; report accuracy on a test set.
",
    "4. Try PCA (2 components) and re-train; compare accuracy.
",
    "5. Write 3 sentences on what preprocessing changed.
"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x",
   "mimetype": "text/x-python",
   "codemirror_mode": {"name": "ipython", "version": 3},
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
