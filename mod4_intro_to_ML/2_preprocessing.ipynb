{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "617d3581",
      "metadata": {},
      "source": [
        "# Intro to ML: Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-0001",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf0b606",
      "metadata": {},
      "source": [
        "## When and Why Preprocess?\n",
        "Preprocessing means **cleaning and transforming raw data** so models can use it.\n",
        "\n",
        "- If data requires **cleaning/standardization** (fix units, handle missing values)\n",
        "- Sometimes gives **better model performance** (features become comparable)\n",
        "- Sometimes gives **faster training** (optimizers behave better)\n",
        "\n",
        "**Analogy:** Convert everyone’s height to the **same unit** before comparing.\n",
        "\n",
        "### Quick Think–Pair–Share\n",
        "1. Your dataset has **age (0–100)** and **income (0–200,000)**. Which feature might dominate a distance-based model, and why?\n",
        "2. Could we feed the strings `\"cat\"`, `\"dog\"`, `\"fish\"` directly into a regression model? What might go wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "when-answers-0003",
      "metadata": {},
      "source": [
        "### A:\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f87275",
      "metadata": {},
      "source": [
        "## Scaling\n",
        "Some algorithms rely on **distances** or **gradients** and care about feature magnitudes.\n",
        "- When features have very **different scales** (e.g., centimeters vs. meters), scale them.\n",
        "- Certain algorithms **perform better/converge faster** if the data is scaled.\n",
        "\n",
        "| Method | What it does | Typical range | Use in this lecture with |\n",
        "|--|--|--|--|\n",
        "| **StandardScaler** | $z = (x - \\mu)/\\sigma$ | mean≈0, std≈1 | KNN, Logistic Regression |\n",
        "| **MinMaxScaler** | maps to [0,1] (default) | [0,1] | KNN, Logistic Regression |\n",
        "\n",
        "*We’ll demonstrate scaling with KNN and Logistic Regression only (the models used in this notebook).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-q-0005",
      "metadata": {},
      "source": [
        "### Q: Why might KNN benefit from scaling more than a model that doesn’t use distances directly?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-a-0006",
      "metadata": {},
      "source": [
        "### A:\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling-demo-0007",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: KNN accuracy WITHOUT vs WITH scaling\n",
        "X, y = datasets.make_classification(\n",
        "    n_samples=600, n_features=3, n_informative=2, n_redundant=0, random_state=42\n",
        ")\n",
        "# Make one feature huge so it dominates Euclidean distance\n",
        "X = X.copy()\n",
        "X[:, 0] = X[:, 0] * 1000\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# KNN without scaling\n",
        "knn_plain = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_plain.fit(X_train, y_train)\n",
        "acc_plain = knn_plain.score(X_test, y_test)\n",
        "print(f\"Accuracy WITHOUT scaling: {acc_plain:.3f}\")\n",
        "\n",
        "# KNN with StandardScaler\n",
        "knn_scaled = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "knn_scaled.fit(X_train, y_train)\n",
        "acc_scaled = knn_scaled.score(X_test, y_test)\n",
        "print(f\"Accuracy WITH    scaling: {acc_scaled:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling-visual-0008",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize big-scale feature BEFORE vs AFTER StandardScaler\n",
        "plt.figure()\n",
        "plt.hist(X[:,0], bins=50)\n",
        "plt.title(\"Feature 0 BEFORE scaling\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "plt.figure()\n",
        "plt.hist(X_scaled[:,0], bins=50)\n",
        "plt.title(\"Feature 0 AFTER StandardScaler\")\n",
        "plt.xlabel(\"Value (z-score)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-reflect-0009",
      "metadata": {},
      "source": [
        "### Q: What changed after scaling, and how might that affect KNN’s distance calculations?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-reflect-a-0010",
      "metadata": {},
      "source": [
        "### A:\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3655400",
      "metadata": {},
      "source": [
        "## Encoding\n",
        "Convert categorical variables into numeric values so models can use them.\n",
        "\n",
        "- **OneHotEncoder**: for categories with **no** natural order (nominal). Example: colors (red/green/blue).\n",
        "- **OrdinalEncoder**: for categories **with** natural order (ordinal). Example: spiciness (low < medium < high).\n",
        "\n",
        "⚠️ If you encode `cat=1, dog=2, fish=3` with an ordinal meaning, a model may treat `fish > dog > cat` as distances it should learn from.\n",
        "\n",
        "### Classroom Check\n",
        "Is `shirt_size` (S, M, L) nominal or ordinal? Is `zip_code` a number or a category in a model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "encoding-a-0012",
      "metadata": {},
      "source": [
        "### A:\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "encoding-toy-0013",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Toy dataset: snacks with a label 'yummy_label'\n",
        "raw = pd.DataFrame({\n",
        "    \"snack\": [\"apple\", \"banana\", \"chips\", \"carrot\", \"chips\", \"apple\"],\n",
        "    \"spiciness\": [\"low\", \"medium\", \"high\", \"low\", \"medium\", \"high\"],\n",
        "    \"price_dollars\": [1.0, 1.2, 2.5, 0.9, 2.7, 1.1],\n",
        "    \"yummy_label\": [1, 1, 0, 1, 0, 1]\n",
        "})\n",
        "raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "encoding-pipe-0014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot for 'snack' (nominal), Ordinal for 'spiciness' (ordered)\n",
        "X = raw.drop(columns=[\"yummy_label\"]) \n",
        "y = raw[\"yummy_label\"]\n",
        "\n",
        "nominal_cols = [\"snack\"]\n",
        "ordinal_cols = [\"spiciness\"]\n",
        "ordinal_order = [[\"low\", \"medium\", \"high\"]]\n",
        "num_cols = [\"price_dollars\"]\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"), nominal_cols),\n",
        "    (\"ord\", OrdinalEncoder(categories=ordinal_order), ordinal_cols),\n",
        "    (\"num\", \"passthrough\", num_cols)\n",
        "])\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"logit\", LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "clf.fit(X, y)\n",
        "print(\"Training accuracy:\", clf.score(X, y))\n",
        "\n",
        "ohe = clf.named_steps[\"prep\"].named_transformers_[\"ohe\"]\n",
        "encoded_names = list(ohe.get_feature_names_out(nominal_cols)) + ordinal_cols + num_cols\n",
        "print(\"Encoded feature names:\", encoded_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "encoding-try-0015",
      "metadata": {},
      "source": [
        "### Try It (2–3 min)\n",
        "1. Add a new snack (e.g., `\"popcorn\"`) to the table and rerun. What happens with `handle_unknown=\"ignore\"`?\n",
        "2. Reverse the `ordinal_order` to `[\"high\", \"medium\", \"low\"]`. How might that change results?\n",
        "3. Swap `LogisticRegression` for `KNeighborsClassifier`. Does performance change?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc14180b",
      "metadata": {},
      "source": [
        "## Principal Component Analysis\n",
        "PCA **distills high-dimensional data** into fewer dimensions (principal components) that capture the most variance.\n",
        "\n",
        "- Useful for **visualization** (2D/3D)\n",
        "- Can speed up models or reduce noise\n",
        "- Components are combinations of original features and may be **harder to interpret**\n",
        "\n",
        "Always **scale** features before PCA so large units don’t dominate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pca-q-0017",
      "metadata": {},
      "source": [
        "### Q: When might losing detail via PCA be okay? When might it be risky?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pca-a-0018",
      "metadata": {},
      "source": [
        "### A:\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pca-iris-0019",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA on the Iris dataset (3 classes available inside this notebook via sklearn)\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "names = iris.target_names\n",
        "\n",
        "pipe_pca = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2))\n",
        "])\n",
        "X_2d = pipe_pca.fit_transform(X_iris)\n",
        "print(\"Explained variance ratio:\", pipe_pca.named_steps[\"pca\"].explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "for label in np.unique(y_iris):\n",
        "    mask = y_iris == label\n",
        "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], label=names[label])\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Iris in 2D via PCA\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pca-compare-0020",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare accuracy with and without PCA (both models are defined in this notebook)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.25, random_state=42)\n",
        "\n",
        "base = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"logit\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "base.fit(X_train, y_train)\n",
        "acc_base = base.score(X_test, y_test)\n",
        "\n",
        "with_pca = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2)),\n",
        "    (\"logit\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "with_pca.fit(X_train, y_train)\n",
        "acc_pca = with_pca.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy without PCA: {acc_base:.3f}\")\n",
        "print(f\"Accuracy with    PCA: {acc_pca:.3f}\")\n",
        "print(\"Note: PCA helps visualization; predictive accuracy may or may not improve.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gotchas-0021",
      "metadata": {},
      "source": [
        "## Common Gotchas\n",
        "- **Data leakage**: Fit scalers/encoders **only on training data**, then apply to validation/test.\n",
        "- **Wrong encoder**: Don’t use ordinal encoding for unordered categories.\n",
        "- **Unscaled PCA**: Scale before PCA.\n",
        "- **High-cardinality one-hot**: Many unique categories can create lots of columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mini-project-0022",
      "metadata": {},
      "source": [
        "## Mini-Project (10–12 min)\n",
        "Work in pairs **using only the data and tools in this notebook**:\n",
        "1. Extend the **snack** table with 4–6 new rows.\n",
        "2. Train a **KNN** classifier to predict `yummy_label`.\n",
        "3. Compare accuracy **with** and **without** scaling numeric columns.\n",
        "4. Try both **OneHot** and **Ordinal** encoding for `spiciness`. Which makes more sense? Why?\n",
        "5. Share one insight with the class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exit-0023",
      "metadata": {},
      "source": [
        "## Exit Ticket\n",
        "1. Why do KNN and logistic regression often benefit from scaling?\n",
        "2. When is `OrdinalEncoder` preferable to `OneHotEncoder`?\n",
        "3. Name one downside of PCA.\n",
        "4. What is **data leakage**, and how do we avoid it when scaling/encoding?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exit-a-0024",
      "metadata": {},
      "source": [
        "### A:\n",
        "YOUR ANSWER HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
